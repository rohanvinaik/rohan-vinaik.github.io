# **Scalable Hybrid Analog-Digital Computing for High-Dimensional Inputs**

## **Introduction**

Designing a **general-purpose hybrid analog-digital computing architecture** that can handle arbitrarily high-dimensional inputs is challenging due to the **combinatorial explosion** of complexity as system size grows. Analog computing exploits continuous physical phenomena (electrical voltages, light intensities, etc.) to perform computations in parallel, offering potentially enormous speed and energy advantages. However, scaling analog systems to many input dimensions or to multi-layer processing often leads to exponential growth in required components and configurations, as well as increased sensitivity to noise and errors. This report analyzes the root causes of this combinatorial explosion in analog circuit and logic design, and surveys strategies to mitigate these issues in **hybrid analog-digital systems**. We compare different types of analog input data (sound, images, sensor signals) for their suitability in scalable analog computing, and evaluate emerging architectures and materials – including neuromorphic circuits, optical processors, quantum-inspired analog solvers, and memristive in-memory computing – that promise robust high-dimensional analog processing. Finally, we discuss the theoretical and practical trade-offs between these approaches, aiming to guide the design of an analog computing framework that can **scale with high-dimensional inputs** without suffering exponential complexity growth.

## **Combinatorial Explosion in Analog Circuit Design**

**Understanding the Explosion:** In digital systems, complexity often scales linearly or polynomially with input size due to the ability to reuse components (e.g. a small set of logic gates used sequentially) and hierarchical design. In analog systems, by contrast, representing or processing additional dimensions frequently demands additional physical components or interconnections that grow *combinatorially*. This “curse of dimensionality” means that as the size of the problem (or number of inputs) increases, the resources required grow **exponentially** . For example, an analog circuit that computes a function of *N* input variables might need dedicated hardware for each pairwise interaction or higher-order term – the number of such terms can explode with *N*. If one naïvely attempted to build an analog logic circuit covering all combinations of *N* inputs (for instance, an analog truth table with continuous values), the circuit complexity would blow up exponentially in *N*. In effect, analog designs often lack the efficient scaling tricks of digital (like time-multiplexing or memory addressing) and instead realize computations with parallel physical processes, which become impractically numerous as dimensionality grows.

**Multilayer Physical Systems:** When analog computations are composed in multiple layers or stages (for example, an analog neural network with several hidden layers, or cascaded analog signal processing stages), the problem is compounded. Each layer’s analog components and their interconnections multiply the total design complexity. A deep analog neural network with full connectivity would require an analog weight (e.g. a resistor, memristor, or transistor circuit) for every connection. For a multilayer network, the number of connections (and thus analog components) grows combinatorially with the number of neurons per layer if fully connected. Beyond sheer component count, *interactions* between layers can create complex analog signal transformations that are hard to design and control – slight parameter changes can produce widely varying outcomes. This complexity is analogous to trying to tune a large, interconnected analog machine: every new element or layer introduces cross-couplings and feedback that grow the overall state-space explosively.

**Implications:** The combinatorial explosion in analog design has several critical implications. First, **physical resource requirements** (number of components, chip area, power) can become prohibitive. A telling comparison: modern FPGAs have tens of thousands of logic cells, whereas field-programmable analog arrays (FPAAs) historically provide only on the order of single-digit analog blocks (e.g. one commercial FPAA had just 4 configurable analog computing cores ). This gap highlights how difficult it is to integrate large numbers of analog computing elements – digital scales into the millions/billions of transistors, while analog complexity hits practical limits much sooner. Second, as analog systems grow, **noise, signal degradation, and crosstalk** tend to accumulate. Unlike digital signals which are regenerated as clean 0/1 levels at each logic gate, analog signals are continuously variable and can suffer attenuation or distortion at every stage. In a large analog network with many stages, errors can quickly accumulate and overwhelm the computation. For instance, an optical analog neural network using many cascaded Mach-Zehnder interferometers (MZIs) sees errors from each device compound as light progresses through the network . Without intervention, a large analog system might produce very imprecise results due to these accumulated analog imperfections. Third, **calibration and design complexity** increase drastically. Each analog component may require tuning (e.g. bias voltages, gain settings) to achieve the desired overall behavior. The more components and interdependencies, the more painstaking the calibration. This is one reason general analog computers of the past were limited in scale – beyond a certain point, it’s too complex to manually tune every knob for a high-dimensional problem. Finally, the combinatorial explosion makes **general-purpose analog computing** a daunting prospect. Traditional analog computers were often tailored to specific differential equations or models; a truly general-purpose analog processor would need to flexibly represent a wide range of high-dimensional computations, which verges on infeasible without clever strategies because a direct analog implementation of an arbitrary high-dim function might require astronomical hardware. In summary, the root cause is that analog computations lack a compact scalable abstraction (everything is “baked into” physical continuous interactions), leading to exponential growth of complexity and diminishing robustness as dimensionality increases.

## **Strategies to Mitigate Complexity in Hybrid Analog-Digital Systems**

Despite these challenges, several strategies can **mitigate combinatorial explosion** in analog design when combined with digital methods in hybrid architectures:

* **Hierarchical & Modular Design:** Breaking a high-dimensional analog computation into smaller sub-systems can prevent the need for a fully combinatorial analog network. A complex task can be divided into modules that handle subsets of inputs or partial computations, with digital logic coordinating their interaction. Instead of one monolithic analog monster circuit, we use many moderate-size analog blocks. Digital interconnects (or time-multiplexed analog signal routing) can link these modules hierarchically. This approach contains complexity by localizing analog interactions. For example, in analog neural networks one might use smaller crossbar arrays for each layer or layer segment, and use digital circuitry to pass intermediate signals between layers (possibly converting to digital in between, or time-multiplexing analog signals). The hybrid BrainScaleS-2 neuromorphic platform follows this idea: it contains **analog cores for neuron/synapse computations** coupled with a digital routing network for communication between cores . By using digital event packets to communicate spikes between analog neuron arrays, BrainScaleS-2 confines analog processing to manageable local blocks (neurons only talk to others via an address-event scheme, not an amorphous fully analog web). This hierarchy prevents the analog hardware complexity from exploding – the system can tile more neuromorphic cores and connect them via digital channels, rather than building one giant analog fabric.

* **Time-Multiplexing and Reconfiguration:** Another powerful method to avoid a combinatorial hardware blow-up is to reuse analog components sequentially for multiple computations. Instead of physically duplicating hardware for every input dimension or every operation, a hybrid system can rapidly reconfigure or time-share a smaller number of analog units. **Field-Programmable Analog Arrays (FPAAs)** exemplify this: they provide a limited pool of analog operators (integrators, amplifiers, multipliers) which can be programmed and reused for different parts of a computation at different times. By cleverly scheduling the analog operations, one can handle high-dimensional input data piecewise rather than concurrently. For instance, an analog signal processor might process an image one row at a time through the same analog filter circuit, rather than having a separate analog filter for every pixel simultaneously. The digital system controls the multiplexing (using analog switches, sample-and-hold circuits, etc., to route different signals into the analog computation blocks in sequence). This trades off time for hardware: it avoids an exponential hardware count at the cost of taking multiple cycles to handle all inputs. As long as the analog computation is fast (which it often is, e.g. analog circuits operate at the speed of electricity or light), the overall throughput can still be high. Reconfiguration can also mean adapting analog hardware “on the fly” to compute different functions as needed, akin to how FPGAs reconfigure logic – thus a limited analog resource can emulate a much larger analog network over time.

* **Hybrid Representation & Partial Digitization:** Pure analog representation of all aspects of data can be relaxed. Often a **mixed-signal approach** is more scalable: some aspects of the information are kept analog (to exploit parallel computation or high bandwidth), while others are quantized or encoded digitally (to exploit noise-resistant storage and flexible routing). For example, one might represent continuous variables as analog voltages but use digital *addresses* or indices to select which variables interact. In neuromorphic chips, neuron membrane potentials can be analog, but neuron addresses (identities) are digital, and spike events are digital pulses. This limits analog interconnect explosion because you don’t need an analog wire connecting every pair of neurons; instead, a digital bus conveys the spike events to targets. Another hybrid representation is using analog computing for **matrix multiplications** or vector operations, but using digital logic to handle program flow and non-linear activation functions. Many analog AI accelerators follow this template: an analog crossbar computes weighted sums in one shot, then results are digitized for thresholding or routing to the next layer. By inserting digital conversions at strategic points, the system prevents uncontrolled analog error growth and can reset error accumulation.

* **Digital Calibration and Error Correction:** A key role for digital components in a hybrid system is to continuously **monitor, calibrate, or correct** the analog components. High-dimensional analog systems can be stabilized by wrapping a digital control loop around them. Digital calibration can periodically measure analog outputs and adjust biases or apply corrective offsets. Recent research emphasizes this for analog neural networks and in-memory computing. For instance, a 2024 Science paper demonstrated a protocol for programming memristor-based analog circuits to achieve *high precision* computing by using multiple low-precision devices in combination . In their approach, the analog memory elements (memristors) are inherently imprecise, but the system uses a clever encoding (summing several memristors’ analog values with weighting) and a digital optimization routine to program them such that together they represent numbers with *arbitrarily high precision* within the limits of digital control . This kind of **codesign of analog with digital algorithms** can counteract analog device variability and noise. Another example is the **error correction in optical analog computing**: MIT researchers recently showed that by modifying the design of optical interference units and using implicit error-correction techniques, scaling an optical neural network actually *reduced* its error . They introduced a 3-beam-splitter interferometer design that made it easier to calibrate each unit’s transfer function, preventing cumulative loss, so that larger optical networks didn’t suffer the usual explosion of error . While their solution was primarily a hardware tweak, the principle extends to hybrid schemes: digital control logic could dynamically adjust analog photonic elements to cancel errors (so-called *self-healing* circuits). In short, **digital oversight** can tame an analog system’s complexity: the analog portion provides the raw computational muscle, and the digital portion keeps it within correct operating bounds via calibration, feedback, and occasionally resetting or re-initializing analog states to prevent drift.

* **Algorithm-Hardware Co-Design:** Tackling combinatorial explosion may also require rethinking algorithms to fit analog’s strengths. Rather than mapping a brute-force high-dimensional algorithm directly to analog hardware, one can design algorithms that factor the problem or use analog-friendly iterative methods. This co-design goes hand in hand with hybrid hardware. For example, if a physical analog solver converges to the solution of a problem (like an analog circuit that naturally finds minima of an energy function), one can choose algorithms that are solved via energy minimization (e.g. analog implementations of gradient descent or heuristics for optimization) so that the analog hardware isn’t forced to enumerate exponentially many states but rather *dynamically* relaxes to a solution. **Machine learning methods are increasingly used to manage complex design spaces** as well – a notable case being Google’s use of reinforcement learning to handle chip floorplanning. Chip layout is a combinatorial optimization nightmare that traditionally took human engineers months, yet Google’s RL-based “AlphaFloorplan” (Mirhoseini and Anna Goldie et al.) demonstrated the ability to search this huge design space in a few hours . The RL agent effectively learned heuristics to navigate the combinatorial possibilities. Similarly, ML could assist analog architecture design – for instance, suggesting how to partition an analog system or tune analog circuit parameters to avoid regions of instability. While not an explicit hardware feature, this approach **offloads complexity to computation (during design or configuration)** so that the final analog system need not brute-force everything. In essence, by intelligently exploring configuration space (with evolutionary algorithms or RL), we can find analog designs or settings that achieve the desired high-dimensional function with far fewer components than an exhaustive approach would require. This is a promising strategy to overcome the *design* side of the combinatorial explosion.

Using these strategies, hybrid analog-digital systems can substantially **overcome the naive exponential scaling** that pure analog implementations would suffer. The analog part is used for what it does best – fast, parallel, continuous operations on data – but constrained or orchestrated by digital logic to prevent runaway complexity and to ensure robustness. The result is a balance: we sacrifice some of analog’s total parallelism in exchange for manageability, often achieving *polynomial* scaling in hardware instead of exponential, and introducing means to handle noise/variability. In the next sections, we examine how different **types of analog data** play into these considerations and then survey specific architectures that embody these principles to achieve scalable analog processing.

## **Analog Data Types and Scalability Considerations**

Not all analog data is equal when it comes to scalable processing. The nature of the input data (its dimensionality, bandwidth, and structure) affects how feasibly it can be processed in analog form at large scale. Below we compare **sound/audio signals**, **image signals**, and **continuous sensor inputs** in terms of their suitability for analog computing, focusing on robustness and integration complexity:

| Analog Data Type | Dimensionality & Structure | Robustness to Analog Processing | Ease of Integration into Analog Circuits |
| ----- | ----- | ----- | ----- |
| **Audio (Sound Waves)** | 1D continuous signal (time-series); moderate bandwidth (Hz–kHz for audio, up to MHz for RF). Typically a single or few channels (e.g. mono, stereo). | **High robustness:** Audio can tolerate a degree of noise or distortion (e.g. analog warmth in music is even desirable). Many analog signal processing techniques (filters, amplifiers) are well-developed for audio/RF. Convolution (filtering) of audio signals in analog (using op-amp filters or analog delay lines) is effective and doesn’t inherently degrade the signal beyond some noise floor. | **Easy integration:** Sound is naturally analog and transduced directly to voltage (microphones) or current. Analog circuits (mixers, equalizers, modulators) handle audio/RF readily. Scaling to multiple audio channels is linear (one filter per channel, etc.) which is manageable. The physical size of audio analog components is small, and low-frequency analog signals are easily routed on a chip or board. Overall, audio processing has a long history of analog implementations, so it integrates comfortably. Problems only arise if we consider extremely high channel counts (e.g. massive sensor arrays or hundreds of audio streams), which might then need hybrid techniques to manage interconnections. |
| **Images (Visual Signals)** | 2D spatial signals (potentially 3D with color); very high dimensionality (e.g. a 1080p image has \~2 million pixels). Each “point” in the image is a data element, so parallel processing implies millions of analog values at once. Bandwidth per pixel is not high (video \~60 Hz frame), but number of parallel channels is huge. | **Moderate robustness:** Optical analog processing can handle images with minimal loss – e.g. using a lens for Fourier transforms is essentially lossless and avoids quantization . However, electronic analog processing of images (e.g. voltage representing pixel intensity) is prone to noise and blur. Small analog errors can visibly degrade an image (introducing blurriness, ghosting, or color shifts). Stacking multiple analog image processing steps (convolution, edge detection, etc.) can compound errors, similar to making multiple analog copies of a photograph. Thus, analog image computing must be carefully designed (often leveraging optical methods) to remain robust. | **Challenging integration:** Handling an entire image in analog means having hardware for potentially millions of parallel channels (one per pixel or per feature). This is infeasible in traditional electronics – instead, analog image processors rely on *optical computing* (the image itself is the analog signal processed by lenses or spatial light modulators). Optical processing is naturally parallel for images (light rays in parallel) and has been used for analog Fourier filtering and correlators for decades. Electronic analog VLSI for images has been demonstrated on smaller scales (e.g. cellular neural network chips of size 32×32 or 64×64), but scaling to megapixel arrays is difficult due to area and mismatch. Typically, image sensor outputs are quickly converted to digital for flexible processing. Thus, while analog **optical** computing can handle high-res images (because light can be manipulated in parallel), integrating that into an electronic analog-digital system is complex (needs optoelectronic interfaces). For electronic circuits, a more practical approach is to use analog processing only at the sensor front-end (e.g. pixel-level amplification or compression in CMOS sensors) and do global image operations digitally, or use *time-multiplexed analog* processing on smaller chunks of the image at a time. |
| **Continuous Sensor Inputs** (e.g. temperature, pressure, biomedical signals) | Typically multiple distinct 1D signals from various sensors; each is low bandwidth (Hz-kHz) and relatively low-dimensional individually. However, in aggregate a system might have dozens or hundreds of such sensors (e.g. an array of environmental sensors). | **High robustness (per sensor):** Analog front-end circuitry (amplifiers, filters) for individual sensors is standard and can be very accurate (low noise amplifiers, etc.). These slow-changing signals are easy to maintain with high fidelity in analog form over a few processing steps (e.g. analog integration or thresholding). There is not much “convolution” between different sensor channels unless explicitly introduced – usually each sensor’s data is independent. Thus, analog processing might involve combining sensor readings (summing, comparing) which analog circuits can do reliably (op-amp summers, etc.). The main issue is interference between channels if they are physically close or share wiring – careful design can minimize crosstalk. | **Moderate integration ease:** For a small number of sensors, analog processing is straightforward – one can build an analog circuit per sensor or a multiplexing circuit to scan them. The challenge grows if there are **many sensors** or a need for complex cross-sensor computations. Wiring up a hundred analog sensors to interact is cumbersome; this is often where digital microcontrollers take over (digitize each sensor and combine in software). However, some sensor network applications use analog techniques like current-mode summing to combine many sensor signals (for example, an analog averaging of many inputs). Generally, each sensor node can have an analog front-end (which scales linearly with sensor count) and then a shared analog processor can time-share operations. The physical size and power of analog front-ends is usually small, so integrating even tens or hundreds on an ASIC is feasible (common in mixed-signal ICs for instrumentation). The limitation is if we wanted **arbitrary interactions** between 100 analog sensor signals – that would require a matrix of analog coupling potentially of size 100×100, which is complex and likely would be done using a time-multiplexed strategy or by converting to digital addresses for routing. In summary, continuous sensor signals are naturally analog and easy to interface, but combining too many of them in one analog domain demands careful architecture (often a hierarchical or hybrid approach). |

**Summary:** Low-dimensional continuous signals (audio, individual sensors) are very amenable to analog processing – one can preserve their fidelity and perform useful computations (filtering, amplification, simple sums) with minimal overhead and without digitization, leveraging analog’s efficiency. High-dimensional signals like large images push the limits of analog integration; purely analog processing of a high-res image would require either an optical system or an infeasible number of electronic components. Thus, for images, **optical analog methods** or **hybrid analog-digital pipelines** (analog preprocessing followed by digital) are preferred to handle the data volume. Audio and similar 1D signals can be fully processed in analog if desired, since scaling to (say) a few dozen filters or mixing many audio channels is manageable (indeed analog audio mixers with tens of channels exist). But if the number of channels becomes very large or if complex transformations are needed (like adaptive filtering, beamforming with large microphone arrays), digital control or partitioning becomes important to avoid noise and complexity. In all cases, **analog pre-processing** at the sensor level (where the data is naturally analog) can reduce the burden on subsequent digital systems – for instance, an analog edge detection circuit at the focal plane of a camera can dramatically compress image data by outputting only salient features, thus coping with the high input dimension by reducing it early. Each data modality thus has an optimal division of labor between analog and digital, balancing the strengths of analog (continuous high-bandwidth transformations) with the strengths of digital (precision, reconfigurability) to achieve scalability.

## **Promising Paradigms for Scalable Analog Processing**

To design a scalable analog computing framework, one can draw on several **emerging architectures and novel materials** that have been the focus of recent research. These paradigms attempt to harness analog computation in innovative ways while addressing the issues of robustness and integration. Below, we evaluate some of the most promising approaches:

### **Neuromorphic Analog and Hybrid Systems**

Neuromorphic computing takes inspiration from the brain to develop circuits that mimic neurons and synapses. Many neuromorphic platforms use **analog circuitry** to model the analog behavior of neurons (such as membrane potential integration) and sometimes even synaptic weights, combined with digital communication (spikes) – a inherently hybrid design. The brain itself can be seen as a highly successful analog-digital hybrid: dendritic and synaptic computations are analog, but spikes are digital pulses, enabling reliable long-range communication without accumulating analog noise. Neuromorphic engineers emulate this to achieve scalability.

Modern examples like **BrainScaleS-2** (Heidelberg University) implement arrays of analog neuron circuits on silicon, with each neuron’s behavior governed by analog differential equations, while using a digital event router to connect neurons and a digital processor to manage plasticity (learning) rules . By accelerating neuron dynamics (BrainScaleS runs neurons 1000× faster than biology in analog mode) and using parallel analog synapse circuits, such systems can simulate large spiking networks efficiently. The key to scaling is that neurons are only locally connected in analog – the long-range or arbitrary connectivity is handled by time-multiplexed routing of spike events (digital packets). This avoids having a fully analog N×N crossbar for N neurons; instead, an address-event scheme routes spikes to target neurons one at a time, much like the brain’s sparse firing avoids every neuron directly analog-coupling to every other. **Mixed-signal design** in neuromorphic chips allows thousands of neurons and synapses on a chip, and multiple chips can be interlinked. Another example is Intel’s **Loihi** neuromorphic chip (though Loihi’s internals are mostly digital, its design philosophy of sparse events and core-level modularity aligns with analog scalability principles). There are also analog CMOS implementations of entire neural networks (both spiking and non-spiking); for instance, researchers have built analog deep neural network accelerators where synaptic weights are stored as analog values (capacitor charges or transistor conductances) and multiplied with neuron voltages. These systems use analog summing amplifiers to sum currents, achieving physical **multiply-accumulate in O(1) time regardless of vector size** – a huge parallel speedup. By using architectures reminiscent of brains (layers of neurons with local connectivity and occasional long-range connections handled via time-multiplexing), neuromorphic analog systems avoid combinatorial interconnect explosion. **Local analog computations** (which can be made robust via careful circuit design and calibration) paired with **global digital orchestration** (to handle routing and plasticity) make this approach highly scalable. The trade-off is that such systems often compute in an *unconventional* way (spikes and membrane dynamics, rather than deterministic digital logic), which suits certain tasks like pattern recognition or solving differential equations but can be less straightforward to use for arbitrary algorithms. Nonetheless, neuromorphic architectures are at the forefront of energy-efficient high-dimensional analog computing, as they demonstrate how to use massive parallelism (many small analog processors operating concurrently) without losing control of complexity.

### **Analog Optical Computing**

Optical computing leverages photons to perform computations at the speed of light, and it inherently supports high-dimensional parallel operations. Light beams can propagate without interfering (to first order) – allowing, for example, an optical lens or diffraction pattern to perform a 2D Fourier transform on an entire image plane in one instant. This **wave-based parallelism** is extremely attractive for analog computing on high-dimensional data like images or large matrices. Optical analog processors have seen a resurgence, especially for accelerating linear algebra in AI workloads. One prominent approach uses networks of Mach-Zehnder interferometers to create optical neural network layers: each MZI can act as a tunable analog multiplier/adder for signals (using phase interference to multiply inputs by a complex weight). By programming a mesh of MZIs, one can implement a matrix multiplication optically, achieving potentially orders-of-magnitude speedups over electronic matrix multiplies and with lower energy (no Joule heating in moving photons) . This concept has been used to demonstrate optical *inference* of neural networks, and research is ongoing to scale it up. The challenge for scaling optical analog systems is **error accumulation and fabrication variability** – large optical circuits may suffer from phase errors, scattering losses, or component mismatches that degrade accuracy. Remarkably, recent work showed that through clever design, these errors can be mitigated even as the system scales. By introducing an additional beam-splitter (a 3rd splitter in each interferometer), researchers enabled calibration of the MZI mesh such that **larger optical networks had lower error rates** (the extra splitter allowed each stage to reach an ideal transfer function, preventing cumulative loss) . This kind of *hardware-based error correction* is analogous to adding redundancy to make analog optical computing more robust and scalable.

Beyond interferometer meshes, there are other optical analog paradigms: **diffractive optical neural networks** use layers of passive diffraction patterns (e.g. 3D printed surfaces or spatial light modulators) to perform matrix operations as light passes through – they essentially “compile” a network’s weights into the physical transmission/reflection properties of surfaces. These can handle enormous parallel data (million-pixel images) instantly. The drawback is they are largely fixed once fabricated (or slow to reconfigure if using e.g. SLMs), meaning they are more application-specific analog accelerators than general-purpose computers. Another exciting area is **photonic reservoir computing** and **time-delay photonic networks**, where a single nonlinear optical node with delayed feedback can act like a high-dimensional system (using time multiplexing of signals in one photonic circuit to emulate many virtual neurons). This leverages the fact that light can have many modes (wavelength channels, polarization, etc.) to pack more information through one physical component. For instance, a microcomb (a laser generating many discrete wavelengths) can encode high-dimensional vectors on different wavelengths, process them through the same optical circuit in parallel, and decode the result – effectively performing a high-dimensional computation in one device using wavelength-division multiplexing .

Optical analog computing is extremely promising for **scaling** because optical systems can naturally handle high bandwidths and dimensionalities (imagine an optical processor acting on an entire optical fiber’s worth of channels – tens of terabits of information – simultaneously). The **trade-offs** are that optical computations are usually linear (or involve specific optical nonlinearities that are hard to integrate at scale), and reprogrammability is non-trivial. Hybrid opto-electronic systems may incorporate digital or electronic control to set up the optical paths (e.g. MEMS or electro-optic modulators controlled by digital logic, as in reconfigurable photonic circuits). Additionally, optical analog computers often need conversion from digital electronic data to optical and back, which can be a bottleneck. However, for tasks like matrix multiplication that dominate many high-dimensional problems, optical analog processors could vastly outperform traditional electronics if their precision and integration issues are solved. As one source notes, analog optical neural networks can run far faster and more efficiently than digital ones for tasks like image classification or speech recognition , provided we can rein in the error rates. With techniques like the 3-MZI design and optical error correction, the **scalability limit of optical computing is being pushed further**, making it a strong candidate for handling massive analog data in the future.

### **Quantum-Inspired Analog Computing**

Quantum computing is often mentioned in the context of combinatorial explosion because a quantum system of *N* qubits inherently explores an exponential $2^N$-dimensional state space. In a sense, quantum computers are the ultimate high-dimensional analog computers – the state of a quantum processor is a continuous amplitude vector in a huge Hilbert space. True quantum computers (gate-based or annealing-based) are still nascent and come with severe coherence and error-correction challenges, but they *demonstrate the principle* that physics can compute in parallel over an exponentially large space. **Quantum-inspired analog computing** refers to using ideas and sometimes hardware from quantum systems to solve hard problems or to achieve parallelism, without necessarily requiring full quantum coherence. One key example is the **Ising Machine**, which is essentially an analog solver for optimization problems mapped to spins in an Ising model (a network of spins with pairwise couplings). The **quantum annealer** from D-Wave is a superconducting circuit that physically implements an Ising model of up to thousands of spins. It uses quantum fluctuations to help the system settle into a low-energy state that encodes the solution to a combinatorial optimization problem. While D-Wave’s machine is quantum to a degree, it can also be viewed as an *analog computer* that uses currents in loops to represent spins and couplings – it finds solutions by physically evolving the system (via annealing) rather than brute-force search. This analog approach can, in theory, evade the explicit combinatorial explosion of checking every configuration, by exploiting the physics to *bias* the search toward good solutions. That said, scaling these machines is challenging: increasing the number of spins or making them fully connected introduces many analog control parameters and possible inaccuracy. In practice, embedding a generic problem onto the hardware graph may require multiple physical qubits per logical spin, etc., which grows complexity. Nonetheless, D-Wave has scaled its devices to over 5000 qubits, showing one path for larger analog systems specialized for certain high-dimensional problems.

Another flavor of Ising machine is the **Coherent Ising Machine (CIM)** using optics: here, the analog computation is done by an optical network of parametric oscillators. Dozens or hundreds of pulses of light (each representing a spin’s state in its phase) circulate in a fiber loop or similar, with an optical coupling implemented to favor certain phase alignments corresponding to problem constraints . These machines have achieved notable success in finding good solutions for large graphs (e.g. MAX-CUT problem) using up to tens of thousands of simulated spins, by virtue of the analog physics naturally exploring many configurations in parallel and “converging” to a minimum. Importantly, researchers have added **optical error correction circuits** to CIMs to improve their reliability as they scale , showing again that even for quantum-inspired analog systems, some form of error mitigation is needed as the system grows.

Quantum-inspired analog computing excels at certain tasks like optimization, sampling, or simulating physical systems, by directly leveraging physical processes. They are **special-purpose** – not general-purpose architectures – but they offer a way to tackle problems that would otherwise blow up combinatorially on regular digital machines. For instance, analog quantum simulators can mimic quantum dynamics of a large molecule more naturally than a digital computer (which struggles with exponential Hilbert space). The **trade-off** is that these systems (quantum annealers, optical Ising machines) often don’t guarantee the optimal solution or exact computation; they give approximate or heuristic results, albeit much faster for some instances. Moreover, controlling a large analog quantum system is a fine art: as with other analog devices, variability and noise (here, decoherence or optical noise) can degrade performance. There’s ongoing work on **hybrid quantum-classical algorithms** where a classical (digital) computer guides a quantum analog machine (like variational quantum eigensolvers guiding a quantum circuit, or a digital feedback loop adjusting parameters of an analog annealer) – effectively a hybrid approach akin to digital calibration in classical analog computing. In summary, quantum-inspired analog machines demonstrate that it’s possible to harness massively parallel physical state spaces without explicitly enumerating states, hinting at how general analog computing might bypass combinatorial explosion for certain problems. They remain a specialized tool, but lessons from them (like using physical dynamics to do computation and carefully correcting errors) are very relevant to general analog architecture design.

### **Memristive and In-Memory Analog Computing**

Memristors (memory resistors) and other non-volatile analog memory devices (RRAM, phase-change memory, etc.) have opened new possibilities for analog computation, particularly in the form of **in-memory computing** architectures. In-memory computing aims to eliminate the separation between processing and memory (the von Neumann bottleneck) by performing computations *within* the memory arrays. Memristive crossbar arrays are a prime example: a crossbar can store a weight matrix in the conductances of its memristors, and by applying input voltages to rows, an analog current summation naturally computes the matrix-vector product $I \= W \\times V$ in one step (Ohm’s and Kirchhoff’s laws do the multiplication and summation). This is highly attractive for neural network inference, linear algebra, and solving systems of equations – tasks which can be mapped to matrix ops. A 64×64 memristor crossbar, for instance, can compute 64 weighted sums in parallel , effectively doing 4096 multiplications and accumulations at once, exploiting analog physics for massive parallelism. By tiling such crossbars, larger dimensions can be handled (many chips now explore 256×256 or 512×512 crossbar blocks).

The scalability of memristive analog computing depends on device uniformity and precision. Early memristor arrays suffered from device variability, drift, and limited precision (each device represented perhaps \~4-8 bits of precision reliably). This limited the size of networks they could accurately implement – errors would accumulate across layers if each multiplication was imprecise. However, **recent advancements have significantly improved memristor array performance and strategies to use them**. For example, researchers built a 4k-memristor (64×64) passive crossbar with \~99% yield and relatively uniform switching characteristics , enabling it to reliably store and compute an image classification task with \<4% error in weight programming . They demonstrated that careful engineering (material improvements, better fabrication for uniformity) can push analog arrays to larger scale. Furthermore, the 2024 Science paper by Xia, Yang et al. introduced a *programming protocol* that achieves high precision from low-precision devices by using multiple devices per weight and optimizing their combined values . Essentially, if one memristor gives you 4 bits of analog precision, use 4 of them to get, say, 10+ bits by summing – a form of redundancy trading hardware for precision, guided by a smart algorithm to minimize overhead. This approach is analogous to bit-slicing in digital (combining multiple low-precision units to get high precision) but done in analog. They report being able to solve PDEs with high precision on their memristor analog compute core , indicating that analog in-memory computing can reach the level of accuracy needed for scientific computing, not just AI inference.

From a complexity standpoint, memristive analog architectures scale by **tiling crossbar blocks** and using peripheral circuits (ADC/DAC, shift-and-add for bit-partitioned computation, etc.) to handle larger vectors/matrices than a single crossbar. The combination of crossbar computing and digital periphery is a classic hybrid strategy: do the bulk of MAC operations in analog (O(1) time, parallel), but use digital logic to orchestrate the flow of data between crossbars and to accumulate partial sums from multiple subarrays. By doing so, one can scale to very high dimensions without an exponential blowup in hardware – it stays roughly quadratic in matrix size for the analog core, plus overhead for the digital interface. Memristive arrays also have the advantage of **non-volatility**, meaning they can store the analog “program” (weights or coefficients) without power. This is helpful in scaling because reprogramming large analog arrays can be time-consuming; keeping the configuration persistent avoids constant re-tuning.

Emerging devices like phase-change memory, ferroelectric FETs, or even analog SRAM can play similar roles. A notable example is **compute-in-memory CNN accelerators** that perform convolution by exploiting analog summation of currents in SRAM bitcells or capacitive charge sharing – these have shown orders-of-magnitude improvements in energy efficiency for large neural nets. New materials (e.g. ferroelectric memristors) promise better linearity and endurance, which directly translates to more scalable analog compute fabrics.

In sum, memristive in-memory analog computing addresses the input dimensionality challenge by *collocating computation with storage*. It cuts down data movement and computes in parallel, so even if you increase the input size, you mostly increase the array size (which grows polynomially with dimension) rather than needing complex control logic. The main limitations to watch are **analog precision and noise**: larger arrays mean more devices contributing noise and more cumulative error, so techniques like the high-precision coding or periodic digital recalibration are essential. But given rapid progress, this approach is very promising for general-purpose acceleration – indeed, it’s **general-purpose approximate computing** in a sense, where many design alternatives open up once you allow some error tolerance . By adjusting how many analog devices you use and how often you digitize intermediate results, you can trade accuracy for efficiency in a controlled way. This makes memristive analog computing a flexible framework that can be scaled up or down to suit the problem’s dimensional needs and precision requirements.

### **Other Noteworthy Approaches**

Beyond the four categories above, it’s worth noting a couple of other angles. **Reconfigurable analog arrays** (large-scale FPAAs) as mentioned are being actively researched to serve as general-purpose analog co-processors. The idea is to provide an analog equivalent to FPGAs where users can program a variety of analog computing tasks (differential equation solving, signal filtering, pattern recognition) on a chip. Progress in this area has been slower than digital FPGAs due to analog’s sensitivity, but work by Hasler et al. has demonstrated large analog arrays with many more configurable elements than earlier FPAAs . If successful, a large FPAA could act as a sandbox for trying different analog computing configurations without fabricating new chips each time – crucial for finding scalable designs.

Also, **biological or chemical analog computing** is an unconventional paradigm where molecules or biochemical networks compute (e.g. DNA computing, or chemical reaction-diffusion processors). These can explore enormous combinatorial spaces in parallel (a test tube has \~$10^{23}$ molecules acting simultaneously), but the difficulty is reading out results and programming such systems. While not the focus of current hybrid electronic design, such approaches are conceptually intriguing for high-dimensional problems like genetic algorithms or graph exploration, albeit with very different practicality and timescales.

## **Trade-offs Between Analog Approaches and General-Purpose Utility**

Designers of a scalable analog computing framework must balance several **trade-offs**. Different analog approaches offer different mixes of speed, precision, energy efficiency, and flexibility. The table below summarizes key pros and cons of various analog computing approaches discussed, particularly in the context of general-purpose vs specialized use:

| Approach | Advantages | Challenges / Trade-offs |
| ----- | ----- | ----- |
| **Classical Continuous-Time Analog (Op-amp based)** – e.g. analog computers solving differential equations, integrator circuits | • **High speed** for solving continuous problems (real-time integration of dynamics).• **Insightful physical analogy** – can directly model physical systems (useful for simulation).• **Information density:** One analog variable can represent a real value with potentially infinite states (limited by noise) – much richer than a single digital bit. | • **Poor scalability:** Each new variable or equation needs additional amplifiers, summing junctions, etc. Hardware grows linearly or worse with problem size.• **Limited reprogramming** – rewiring analog computers or reconfiguring op-amp circuits is cumbersome (modern FPAAs aim to improve this).• **Accumulated error:** Long analog computation sequences drift due to component offsets; not suitable for very deep computation without resets.• Generally not energy-efficient at large scale (static power consumption, calibration overhead). More of a niche for specific simulations in modern times. |
| **Neuromorphic (Analog/Digital Hybrid)** – spiking neuron analog cores with digital routing (BrainScaleS, etc.) | • **Biologically scalable design:** neurons and synapses can be tiled to large networks without all-to-all analog wiring (uses sparse event-driven communication).• **Energy-efficient** for neural network tasks – exploits event sparsity and analog integration (brain-inspired).• **Robustness through redundancy:** Many neurons can fail or be imprecise but the overall network can still function (graceful degradation, like the brain).• Can incorporate learning/plasticity on-chip for adaptive computing. | • **Specialized computation model:** best suited for neural network or differential equation problems. General-purpose computing (like precise arithmetic or logic) is not natural in this paradigm.• **Calibration & mismatch:** analog neuron circuits still need calibration for device mismatch – large arrays require careful tuning or per-neuron digital calibration to ensure consistency. (BrainScaleS includes on-chip ADCs for monitoring analog behavior).• **Throughput vs precision trade-off:** spiking systems are excellent for parallelism and energy, but if a task requires exact numerical precision or deterministic results, the inherent noise and randomness of neuromorphic computing can be a downside.• Tooling and programming models for neuromorphic hardware are still maturing – not as straightforward as writing code for a CPU/GPU. |
| **Optical Analog Computing** – interferometer networks, diffractive optics, etc. | • **Massive parallelism and bandwidth:** can process *entire vectors or images in parallel* (e.g. an optical lens performing a Fourier transform on a 2D waveform in one go) .• **Ultra-fast**: photonic computation occurs at light speed and can be femtosecond-fast for certain operations; potential for THz-scale computing rates for matrix ops.• **Low energy per operation:** no capacitance charging as in electronics; a properly designed passive optical computing step (like light through a lens or passive interferometer) consumes minimal energy.• Natural for **linear algebra** and convolution operations that underlie many AI and signal processing tasks. | • **Precision limitations:** analog optics suffer from scattering, diffraction limits, and component precision (phase errors). Achieving digital-like precision (error \< 0.1%) is hard; analog optical is usually analog *approximate* computing.• **Bulky or integration challenges:** Free-space optics are large; integrated photonics brings it to chip-scale but still larger than electronic circuits and with connectivity constraints (waveguides are less dense than wires).• **Fixed or hard-to-program:** Many optical computing setups are either static once fabricated or slow to reconfigure (e.g. mechanical mirror movements, thermal phase shifters). Dynamic reprogramming often requires interfacing with electronics (losing some speed advantage).• **I/O conversion overhead:** Getting data into/out of optical domain requires DACs (lasers/modulators) and ADCs (photodetectors) which can bottleneck the system. For general-purpose use, this conversion cost must be outweighed by the optical speedup in the core. |
| **Quantum-Inspired Analog** (Quantum annealers, Ising machines) | • **Explores many states in parallel:** By exploiting superposition or an analog collective mode, can “try” an exponential number of configurations implicitly – useful for optimization/sampling problems that blow up on digital computers.• **Problem-specific efficiency:** Excels at certain NP-hard problems (e.g. finding ground state of spin glass) by physically embodying the problem – often finds good solutions faster than brute force would.• For quantum annealing: leverages quantum tunneling to escape local minima, potentially outperforming purely classical annealing in some cases.• Continual improvements in scale (thousands of qubits) suggest these could handle very large input sizes for specific problems. | • **Not general-purpose:** These machines don’t run arbitrary algorithms; they solve a narrow class of problems (combinatorial optimization, Ising model tasks). They’re more like analog co-processors or accelerators than standalone general computers.• **Reliability and readout:** Solutions are probabilistic. You often need to run the machine multiple times to gain confidence in the result or get the optimal answer (especially for quantum ones). This stochastic nature complicates their use in deterministic computations.• **Scaling pain points:** Coupling every pair of spins (fully connected graph) is still infeasible; sparse hardware graphs require problem embedding that can use up many physical qubits per logical qubit. Similarly, optical Ising machines with full coupling need complex optical delay schemes or electronics in the loop to mediate interactions, which can bottleneck as system size grows.• **Decoherence and noise:** Quantum devices need error correction or operate in regimes where analog errors might steer results away from ideal. Even “quantum-inspired” classical analog systems (like optical oscillators) can be sensitive to noise, requiring external stabilization. |
| **Memristive In-Memory Analog** – Crossbar arrays with ADC/DAC peripherals | • **Parallel MAC operations with high density:** A single crossbar can perform thousands of multiplications in parallel in one step – extremely efficient for large matrix-vector multiplications .• **Minimized data movement:** computing in-memory means no long-distance shuttling of data; this is critical as data movement costs dominate in big data/AI tasks . As the problem size grows, the benefit of in-memory (analog) computation grows because it sidesteps the memory bandwidth bottleneck of digital systems.• **Field-programmability (to a degree):** Changing the “program” is as simple as re-writing the memristor weights, which can be done in milliseconds. Not as dynamic as a CPU instruction, but much faster to reconfigure than fabricating a new ASIC. This allows adapting the analog computer to different tasks (within the scope of matrix operations).• **Good integration roadmap:** Memristor crossbars can be made with CMOS-friendly processes and stacked in 3D. They align with existing semiconductor manufacturing, easing the path to larger arrays and commercialization. | • **Precision and noise:** Each analog operation has limited precision (e.g. 8-bit effective or less). To get high precision, often need to split computations (perform them in parts and sum digitally, or use multiple devices per weight as in ). This overhead complicates the design and reduces some of the raw parallel advantage.• **ADC/DAC overhead:** Every analog input and output needs conversion. High resolution ADCs are power-hungry and slow relative to the analog core. For large matrices, the number of ADCs can be large (though typically one per output neuron in neural network use). The energy and area of converters can dominate if not carefully architected (using low-resolution ADCs or time-sharing them can help).• **Endurance and variability:** Writing memristors many times can wear them out. Variations in device behavior mean the same write voltage might set slightly different conductances on different cells, so a lot of **calibration and verification** is needed when programming the array (often an iterative write-verify scheme to get each weight close to target). This overhead increases with array size.• **Non-linearity:** If using analog computing for multiple layers (like an analog neural net with several crossbar layers cascaded), the analog signals may need to be converted to digital or at least re-zeroed in between because the analog accumulations can saturate or deviate due to device non-linear response outside a small signal range. Thus, truly deep analog networks still require hybrid handling (e.g. analog within each layer, digital between layers). |

As the table suggests, there is no one-size-fits-all analog approach – each comes with trade-offs in **precision, flexibility, and applicable domain**. A general-purpose analog computing framework might, in fact, incorporate *multiple* of these paradigms in a heterogeneous design. For example, one could envision a system with memristive crossbar accelerators for linear algebra, analog neurons for event-driven tasks, and digital logic tying everything together and handling parts that require exactness. The overarching theme is that **hybridization is crucial**: combining analog and digital intelligently allows us to exploit analog’s strengths (massive parallelism and energy efficiency) while using digital to compensate for its weaknesses (accuracy, complex control).

Notably, analog computing does **not** magically eliminate combinatorial complexity in a theoretical sense – an NP-hard problem remains hard (quantum computers aside, and even they don’t solve NP-hard problems in polynomial time without conjectured complexity separations). What analog offers is a way to *tackle large-scale instances* of problems more efficiently by doing parallel work in physics. The brain, analog optical processors, and in-memory compute substrates all show that tremendous concurrent analog activity can be harnessed **without** a combinatorial explosion *if* the architecture is well-chosen. The brain doesn’t connect every neuron to every other, optical systems naturally perform only certain operations (like linear transforms) extremely well, and memristive arrays exploit locality and physics to avoid shuffling data around.

**Theoretical vs Practical Perspective:** Theoretically, analog computers can achieve polynomial resource usage for some tasks that might seem to require exponential resources, by exploiting continuous variables (for instance, an analog integrator can solve certain differential equations in constant time that a digital simulation would step through many time points). However, theoretical models like the General Purpose Analog Computer (GPAC) and analog models of computation show that analog devices are subject to limits like noise and bounded precision, which often place them in equivalent complexity classes to digital algorithms when those factors are accounted for. Practically, the goal is not to break Turing computability or complexity limits, but to build a machine that, for a given large problem, uses **far less energy or time** than a conventional computer by leveraging physics. Each of the architectures above should be seen in that light – a means to achieve a more *efficient scaling constant* or lower exponent, rather than to defy exponential complexity in the mathematical sense.

## **Conclusion**

Designing an analog computing framework that scales to high-dimensional inputs is a complex endeavor requiring careful balancing of physics and abstraction. The **combinatorial explosion** in naive analog designs stems from the lack of discrete modularity – every additional degree of freedom demands new hardware and introduces new interactions. By embracing **hybrid architectures**, we can contain this explosion: using digital techniques for control, communication, and correction keeps the analog core in check, while analog processing units deliver unparalleled parallelism for the parts of the computation that benefit from it. The suitability of analog computing varies with data type – one can process many audio signals or a handful of sensor feeds purely in analog, but for megapixel images or very deep logic, analog needs augmentation (optical methods or conversion to digital slices) to remain effective.

Emerging technologies provide a rich toolkit for building such systems. **Neuromorphic analog chips** demonstrate how to scale analog “neurons” to large networks by adopting event-driven communication and redundancy, achieving brain-like robust computation. **Photonic processors** offer a path to handle massive data like images or large matrix math with speed-of-light operations, especially as advances in optical error correction and integration address previous scaling limits . **Quantum and quantum-inspired devices** remind us that exploiting the full range of physical states (quantum superposition or analog oscillator amplitudes) can yield exponential parallelism – hinting that the ultimate analog computer might harness phenomena beyond classical electronics. And **memristive in-memory computing** is bringing analog back into the mainstream of computing architecture, blurring the line between memory and processor to handle high-dimensional linear algebra tasks efficiently . Researchers like Anna Goldie and colleagues have even applied machine learning to optimize hardware design itself, tackling combinatorial search spaces in chip layout ; such approaches could be pivotal in configuring large analog systems which have enormous design flexibility.

In building a scalable analog computer, one should expect to use **approximate computing** principles: analog computations may not be exact, but they can be made “good enough” for tasks like machine learning, signal processing, or simulation, with digital correction layers ensuring the final outputs meet accuracy requirements . The benefit is a significant boost in speed and efficiency by avoiding the exhaustive step-by-step operations of digital logic. The theoretical limits (noise, component variations) mean that purely analog computers likely won’t replace digital for all tasks – instead, the future lies in **hybrid analog-digital architectures** that are **problem-driven**. For a given high-dimensional problem, the architecture can be tailored: e.g., an optical analog front-end to compute convolutions, a memristor array to do classification, and a digital CPU to handle conditional logic and exact arithmetic. This synergy can prevent any one part of the system from becoming the bottleneck due to combinatorial growth.

In conclusion, a robust high-dimensional analog computing framework will incorporate **modularity (to handle arbitrary input dimensionality by extension rather than redesign)**, **error mitigation (to prevent noise from overwhelming large analog systems)**, and **heterogeneity (choosing the right physical medium for each subtask)**. By learning from the brain’s mix of analog and pulse-coded digital signals, and by leveraging cutting-edge research in photonics, memristors, and beyond, we can chart a path toward analog computing engines that scale gracefully. Such systems could transform how we tackle complex computations – offering orders-of-magnitude improvements in performance and energy for certain tasks – all while sidestepping the worst of the combinatorial explosion through clever architecture and hybrid design. The continued progress in this interdisciplinary area suggests that far from being relics of the past, analog principles will play an integral role in the **future of computing**, especially as we push into domains where the data is high-dimensional, the models are complex, and efficiency is paramount.

**References:** The analysis above integrates insights from recent literature and advancements. For instance, the curse-of-dimensionality in conventional computing and the promise of in-memory analog computing are highlighted by Xia *et al.* . Modern analog neural and neuromorphic systems like BrainScaleS-2 illustrate hybrid design for scaling neural networks . Optical computing’s scaling challenges and innovations (such as the 3-MZI interferometer) are documented by Bang *et al.* and MIT researchers . The potential of analog computing in the context of AI acceleration is echoed by industry experts like Fick of Mythic, noting the high information density of analog signals (up to 27 bits on one wire) and the energy benefits , albeit with necessary trade-offs in predictability. By referencing such works and others throughout (including Google’s RL-based chip design and memristor crossbar demonstrations ), we ground the discussed concepts in concrete research progress. These sources collectively paint a picture that while challenges remain, the field is actively developing solutions to make scalable analog/hybrid computing a reality.

