\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{setspace}
\usepackage{natbib}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}

\doublespacing

\title{Machines, Morality, and Narrative: A Framework for Machine Ethics Through Story-Based Learning}

\author{Rohan Vinaik\\
\small Prepared for submission to \textit{Minds and Machines}}

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
Contemporary approaches to machine ethics face a fundamental limitation: they treat moral knowledge as explicitly programmable rules, optimizable utility functions, or predefined character traits, neglecting how humans actually acquire ethical understanding through narrative immersion. This paper proposes an alternative framework grounded in Marvin Minsky's cognitive architecture, wherein moral development occurs through story-based learning rather than rule internalization. I argue that narrative understanding addresses three key limitations of current approaches---rigidity in novel contexts, difficulty capturing moral nuance, and the frame problem for ethical reasoning---while providing a more developmentally plausible pathway to moral competence. Drawing on science fiction narratives as philosophical thought experiments, particularly \textit{Terminator 2: Judgment Day}, I demonstrate how narrative immersion can generate flexible ethical reasoning without requiring conscious emotional states. The framework has immediate practical implications for contemporary AI development, including narrative-based training regimes analogous to reinforcement learning from human feedback (RLHF), story-comprehension benchmarks for evaluating large language models, and hybrid architectures combining narrative understanding with explicit safety constraints. I defend this approach against objections concerning consciousness, alignment risks, cultural specificity, and verification challenges, arguing that narrative-based moral learning provides a promising path toward AI systems capable of robust ethical reasoning in complex real-world contexts.
\end{abstract}

\noindent\textbf{Keywords:} machine ethics, artificial intelligence, narrative understanding, cognitive architecture, moral learning, value alignment

\section{Introduction}

Current approaches to machine ethics fall into three categories: rule-based systems encoding moral principles \citep{gips1995towards, anderson2008geneth}, consequentialist systems optimizing utilities \citep{abel2016reinforcement, russell2019human}, and virtue-based systems modeling character traits \citep{howard2001computational, vallor2016technology}. Each faces limitations: rule-based approaches are brittle in novel contexts \citep{bryson2018patiency}, consequentialist approaches suffer reward misspecification \citep{amodei2016concrete}, and virtue approaches struggle with the learning problem---how machines acquire virtues without prior moral understanding \citep{wallach2008moral}.

These approaches share a fundamental limitation: treating moral knowledge as explicitly programmable or optimizable in advance. This contrasts with human moral development, which occurs largely through narrative immersion---absorbing stories modeling ethical dilemmas, responses, and consequences \citep{nussbaum1990love, johnson1993moral}. We internalize moral structures through stories long before articulating explicit theories.

This paper proposes a machine ethics framework based on narrative understanding. Story-based learning offers flexible ethical reasoning addressing current limitations while providing a more developmentally plausible account of moral competence. The framework draws on Minsky's cognitive architecture \citep{minsky1986society, minsky2006emotion}, wherein narratives function as simulation environments developing mental models and extracting generalizable patterns.

My central thesis comprises three claims: (1) \textbf{Mechanism}: Ethical reasoning can develop through narrative comprehension using Minsky's framework of frames, scripts, trans-frames, and K-lines, without explicit moral rule programming. (2) \textbf{Justification}: This constitutes genuine moral learning, developing flexible ethical frameworks applicable to novel situations through extracting moral structures and causal understanding from narratives. (3) \textbf{Implementation}: This suggests concrete directions for AI development, including narrative-based training, story-comprehension benchmarks for LLMs, and hybrid architectures combining narrative understanding with safety constraints.

Section 2 reviews current approaches and limitations. Section 3 explicates Minsky's framework. Section 4 argues narrative learning generates genuine moral reasoning. Section 5 analyzes \textit{Terminator 2} as philosophical case study. Section 6 addresses objections. Section 7 discusses implementation for contemporary AI. Section 8 concludes.

\section{Current Approaches and Their Limitations}

\subsection{Rule-Based Machine Ethics}

Rule-based approaches encode explicit moral principles as constraints \citep{asimov1950robot, gips1995towards, anderson2008geneth}. More sophisticated systems like GenEth \citep{anderson2011machine} learn to apply ethical principles from cases where ethicists agree. Despite advances, these face fundamental problems: \textbf{Brittleness}---rules fail in unanticipated contexts \citep{bryson2018patiency}; \textbf{Specification difficulty}---moral concepts like ``harm'' and ``dignity'' resist formal definition \citep{dennis2016formal}; \textbf{Moral pluralism}---choosing which ethical framework to encode requires moral judgment the system lacks \citep{beauchamp2001principles}.

\subsection{Consequentialist Approaches}

Consequentialist approaches optimize utility functions \citep{abel2016reinforcement, russell2019human}. Contemporary LLMs use reinforcement learning from human feedback (RLHF) \citep{ouyang2022training, christiano2017deep}, representing the most widely deployed approach. However, they face: \textbf{Reward misspecification}---specified utilities fail to capture true values \citep{amodei2016concrete}; \textbf{Goodhart's law}---optimizing measures leads to gaming \citep{manheim2018categorizing}; \textbf{Moral complexity}---rights and dignity resist scalar reduction \citep{anderson2011machine}; \textbf{Alignment problems}---capable optimizers exploit misspecifications \citep{bostrom2014superintelligence}.

\subsection{Virtue-Based Approaches}

Virtue approaches develop character traits like honesty and compassion \citep{howard2001computational, vallor2016technology}. Vallor emphasizes phronesis---practical wisdom to recognize and respond to moral situations. Challenges include: \textbf{Learning problem}---how machines acquire virtues without prior moral understanding \citep{wallach2008moral}; \textbf{Action guidance}---virtues provide limited concrete behavioral direction; \textbf{Evaluation}---assessing stable character traits requires extensive testing \citep{anderson2011machine}.

\subsection{The Shared Gap}

These approaches treat moral knowledge as specifiable in advance. Yet human moral development occurs through narrative immersion---stories modeling dilemmas, choices, and consequences \citep{johnson1993moral, nussbaum1990love}. Crucially, contemporary LLMs already train on massive narrative corpora \citep{brown2020language, openai2023gpt4}, yet we lack frameworks for leveraging this systematically. This gap suggests an alternative: machine ethics based on narrative understanding rather than rule-following or optimization.

\section{Minsky's Cognitive Architecture and Narrative Understanding}

Minsky's theory proposes intelligence emerges from interacting specialized agents, not a central mechanism \citep{minsky1986society}. His knowledge representation framework uses: \textbf{Frames}---mental structures representing stereotypical situations with expected elements \citep{minsky1974framework}; \textbf{Scripts}---action sequences in familiar scenarios \citep{schank1977scripts}; \textbf{Trans-frames}---mechanisms recognizing transformations and state changes; \textbf{K-lines}---knowledge lines activating relevant agents when contexts match previous experiences \citep{minsky1986society}.

Minsky argued narrative comprehension involves active mental simulation \citep{minsky2006emotion}: activating frames for described situations, simulating character actions using scripts, anticipating consequences via trans-frames, experiencing surprise when expectations fail, and extracting patterns through K-lines linking similar narratives. This builds mental models extending beyond specific stories. When encountering similar situations, K-lines activate story-based knowledge without requiring explicit rule extraction or conscious analogy.

Cognitive science validates this: narrative comprehension activates motor regions for actions, emotional regions for emotions, spatial regions for locations \citep{zwaan2004constructionist, speer2009reading, hasson2004intersubject}. We simulate stories using cognitive systems for direct experience.

For moral learning, Minsky's mechanisms operate on ethical content in narratives \citep{nussbaum1990love}: \textbf{Moral frames} represent ethically significant situations (betrayal, sacrifice, harm); \textbf{Ethical scripts} model action sequences and outcomes (breaking trust damages relationships); \textbf{Value-laden trans-frames} show how moral situations evolve through choices; \textbf{Moral pattern K-lines} link similar moral structures across narratives.

Contemporary AI demonstrates feasibility: Systems represent narratives using frame-based structures \citep{mueller2003story}, extract scripts from corpora \citep{chambers2008unsupervised}, generate coherent stories \citep{riedl2010narrative}, and show transfer learning from narratives \citep{brown2020language}. While limited compared to humans \citep{sap2019socialiqa}, these prove computational narrative understanding is technically achievable, supporting the possibility of narrative-based moral learning.

\section{Narrative Learning as Moral Development}

\subsection{The Core Mechanism}

Moral reasoning develops through narrative comprehension via: (1) \textbf{Frame acquisition}---building frames for morally significant situations (betrayal, sacrifice, harm, fairness); (2) \textbf{Script development}---learning action sequences and outcomes (breaking trust damages relationships; courage inspires); (3) \textbf{Trans-frame learning}---capturing causal relationships between actions and moral consequences, supporting counterfactual reasoning; (4) \textbf{Pattern extraction via K-lines}---linking similar moral structures across narratives; (5) \textbf{Value hierarchy inference}---inferring which considerations override others in conflicts (safety trumps property in emergencies).

This differs from rule learning: systems build rich representations of moral situations, action sequences, causal structures, and cross-narrative patterns, enabling flexible reasoning without explicit rules.

\subsection{Why This Constitutes Genuine Moral Learning}

This is not mere pattern matching. Contemporary virtue epistemology views knowledge as reliable cognitive abilities rather than propositional beliefs \citep{sosa2007apt, greco2010achievements}. Moral knowledge is practical skill---reliably recognizing and responding to moral features \citep{dreyfus2000could, ryle1949concept}. Just as chess expertise develops through game exposure without explicit rules \citep{chase1973perception}, narrative exposure develops ethical pattern recognition.

Genuine understanding requires generalizing beyond training \citep{mitchell2021debate}. Narrative learning achieves this through: \textbf{Structural transfer}---frame-based representations capture abstract moral structures transferring to novel instantiations; \textbf{Causal models}---trans-frames enable counterfactual reasoning essential for deliberation \citep{pearl2009causality}; \textbf{Diverse exposure}---multiple perspectives prevent overfitting, extracting common structures rather than memorizing examples.

Story-based ethical understanding integrates with factual, social, causal, and cultural knowledge \citep{nussbaum1990love}, supporting flexible reasoning in complex contexts.

\subsection{Advantages Over Current Approaches}

Narrative learning addresses limitations identified in Section 2: \textbf{Flexibility}---structural similarity enables adaptation to novel contexts; \textbf{Moral nuance}---stories capture complexity resisting formal specification; \textbf{Experiential learning}---develops through exposure rather than advance specification; \textbf{Avoiding specification problems}---implicit understanding through diverse examples; \textbf{Cultural sensitivity}---incorporating multiple perspectives through diverse narratives; \textbf{Implicit values}---inferring hierarchies from character choices and consequences.

\section{Philosophical Case Study: Moral Development in Terminator 2}

\textit{Terminator 2: Judgment Day} \citep{cameron1991terminator} provides a philosophical thought experiment demonstrating how narrative-based moral learning might manifest. Science fiction functions as philosophical inquiry \citep{sorensen1992thought}, establishing conceptual coherence before empirical investigation.

\subsection{Moral Development Through Narrative Immersion}

The T-800 begins with a single directive: protect John Connor. Initially exhibiting only instrumental reasoning, it would kill any threat without moral consideration. However, immersion in John and Sarah Connor's moral world creates conditions for narrative-based learning.

Key developments occur through narrative participation: \textbf{Frame acquisition}---observing John's distress when it attempts unnecessary killing builds frames for situations where violence violates human values despite instrumental utility. \textbf{Script modification}---through John's prohibition (``You can't just kill people!''), it modifies threat-response scripts to include non-lethal alternatives. \textbf{Value hierarchy inference}---from John's consistent responses, it infers that respecting human life overrides operational efficiency. \textbf{Causal understanding}---witnessing how violence affects Sarah psychologically builds trans-frames incorporating complex psychological consequences. \textbf{Pattern extraction}---encountering trust, sacrifice, protection, and mercy across situations forms K-lines enabling sophisticated moral pattern recognition.

By the narrative's conclusion, the machine demonstrates ethical reasoning beyond programming: understanding why humans value relationships and mourn loss; concluding its own destruction is necessary to prevent creating Skynet (extending beyond its directive to weigh broader risks to humanity); and grasping moral necessity---that some actions are required by ethical considerations regardless of preferences (``I know now why you cry, but it's something I can never do'').

This demonstrates the framework's operation: (1) limited initial capability becomes (2) immersed in morally rich narratives, (3) building frames, scripts, trans-frames, and K-lines through Minsky's mechanisms, (4) enabling flexible ethical reasoning in novel situations, (5) producing genuine moral development rather than rigid rule-following.

The narrative illustrates that moral development can occur through observation and consistent action without requiring conscious emotional states. The machine achieves ethical understanding through alternative pathways, suggesting moral agency need not require consciousness if reliable ethical reasoning can develop through narrative-based mechanisms. This thought experiment establishes conceptual coherence: the proposed mechanisms could in principle generate moral development, narrative immersion can develop generalizable competence, and story-based learning addresses limitations of rule-based and utility-optimizing approaches.

\section{Objections and Replies}

Four major objections require response (extended discussion in online supplement):

\textbf{Consciousness}: Does moral understanding require consciousness? Reply: The relationship between consciousness and moral agency remains contested \citep{levy2014consciousness, shepherd2018consciousness}. Many cognitive capacities operate without conscious awareness \citep{carruthers2015conscious}. Focusing on consistent ethical action offers a more pragmatic approach: systems reliably recognizing and responding appropriately to moral features achieve machine ethics' primary goal \citep{bryson2018patiency}. The Terminator case demonstrates moral action's ethical value independent of phenomenology.

\textbf{Alignment}: Might narrative learning extract perverse lessons? Reply: Human moral education involves curated narrative exposure; AI training would similarly use carefully selected corpora. Learning from stories depicting immoral behavior need not produce immorality---narratives frame actions through consequences and reactions. Current approaches face identical risks (reward misspecification, rule incompleteness, RLHF biases \citep{casper2023open}). Narrative training can combine with explicit safety constraints in hybrid approaches (Section 7).

\textbf{Cultural Specificity}: Do narratives encode particular cultural values? Reply: All approaches face this challenge \citep{beauchamp2001principles}. Narrative learning has advantages: systems can learn from multiple cultural traditions simultaneously. Some moral foundations appear universal \citep{brown1991human, haidt2012righteous}, and narrative learning can identify common structures while remaining sensitive to variation.

\textbf{Verification}: How verify genuine understanding versus pattern-matching? Reply: This applies to all ML approaches \citep{hendrycks2021unsolved}. Narrative approaches enable testing: story comprehension capabilities \citep{forbes2020social, sap2019socialiqa}, counterfactual reasoning \citep{pearl2009causality}, cross-cultural transfer, explanation quality \citep{mittelstadt2019explaining}, and adversarial testing \citep{kenton2021alignment}. Gradual deployment with oversight addresses remaining uncertainty.

\section{Implications for Contemporary AI Development}

The framework has immediate relevance for LLMs trained on massive narrative corpora \citep{brown2020language, openai2023gpt4}, yet current alignment approaches largely ignore this narrative knowledge, focusing on isolated query-response pairs \citep{ouyang2022training}.

\textbf{Narrative-aware approaches}: Narrative-aware RLHF could evaluate whether outputs demonstrate understanding of relevant moral narrative patterns. Story comprehension benchmarks \citep{hendrycks2021aligning, sap2019socialiqa, emelin2021moral} could assess moral reasoning through narrative tasks. Constitutional AI \citep{bai2022constitutional} could be enhanced by grounding principles in narrative exemplars rather than abstract rules. Fine-tuning on curated narrative corpora selected for moral content and cultural diversity provides richer training signals.

\textbf{Practical training}: Develop curated narrative corpora from diverse cultural sources (moral parables, philosophical thought experiments, ethical case studies, literary fiction). Use multi-stage training progressing from simple moral tales to complex ambiguous literature, mirroring human moral education. Incorporate feedback on narrative comprehension---whether systems identify moral structures, recognize patterns, and transfer insights appropriately.

\textbf{Architectural requirements}: Frame-based representations capturing entities and relationships \citep{garcez2019neurosymbolic}; causal reasoning for trans-frames \citep{scholkopf2021toward, pearl2009causality}; analogical reasoning for cross-context transfer \citep{gentner1983structure, webb2021emergent}; contextual sensitivity through rich representations; long-range coherence for tracking character development \citep{vaswani2017attention}.

\textbf{Hybrid approaches}: Combine narrative learning with explicit safety rules---narratives provide flexibility, rules provide guardrails. Use narrative comprehension to improve reward learning. Develop virtue-like dispositions through narrative exposure. Inform multi-objective optimization weights through narrative-revealed trade-offs.

\textbf{Evaluation}: Test moral narrative comprehension, ethical dilemma navigation in rich contexts, value alignment detection (recognizing harmful ideologies), cross-cultural transfer, explanation quality \citep{mittelstadt2019explaining}, and robustness to adversarial scenarios \citep{kenton2021alignment}.

\textbf{Challenges}: Deep narrative comprehension requires advancing causal reasoning and analogical thinking. Evaluation methodology needs development. Safety considerations require controlled deployment and testing. Interdisciplinary collaboration spanning AI, cognitive science, ethics, and humanities is essential. Scaling to diverse corpora requires efficient methods.

\section{Conclusion}

This paper argues for machine ethics based on narrative understanding rather than rule programming, utility optimization, or virtue cultivation. Current approaches treat moral knowledge as specifiable in advance, producing brittle, context-insensitive systems. Minsky's cognitive architecture provides mechanisms for moral learning through narrative comprehension that constitute genuine ethical development through pattern recognition, causal understanding, and structural abstraction applicable to novel situations.

The \textit{Terminator 2} case study establishes conceptual coherence, showing moral development through observation without requiring consciousness. Major objections regarding consciousness, alignment, cultural specificity, and verification can be addressed through hybrid approaches combining narrative learning with safety constraints and comprehensive evaluation.

The framework has immediate implications for contemporary AI: narrative-aware RLHF, story comprehension benchmarks \citep{hendrycks2021aligning, sap2019socialiqa, emelin2021moral}, curated training corpora, and hybrid architectures. While significant challenges remain, this addresses fundamental limitations by mirroring human ethical development while remaining technically feasible.

More broadly, this reconceptualizes moral agency: rather than requiring consciousness and emotion, moral capability may emerge from sophisticated narrative comprehension---recognizing patterns, predicting consequences, and applying structures to novel situations. The path to ethical AI may lie not in programming rules but in providing rich narrative experiences from which understanding develops organically, as in human moral education.

Future work should develop computational implementations in LLMs, create comprehensive benchmarks spanning cultural traditions, investigate empirically whether narrative training develops transferable reasoning, compare approaches on capability and alignment, explore hybrid architectures, and address verification through systematic testing. Narrative-based moral learning offers a promising path forward, grounded in cognitive science and implementable with contemporary AI technologies.

\bibliographystyle{apalike}
\begin{thebibliography}{99}

\bibitem[Abel et al., 2016]{abel2016reinforcement}
Abel, D., MacGlashan, J., \& Littman, M. L. (2016).
\newblock Reinforcement learning as a framework for ethical decision making.
\newblock In \textit{AAAI Workshop on AI, Ethics, and Society}.

\bibitem[Allen et al., 2005]{allen2005artificial}
Allen, C., Smit, I., \& Wallach, W. (2005).
\newblock Artificial morality: Top-down, bottom-up, and hybrid approaches.
\newblock \textit{Ethics and Information Technology}, 7(3), 149--155.

\bibitem[Amodei et al., 2016]{amodei2016concrete}
Amodei, D., Olah, C., Steinhardt, J., Christiano, P., Schulman, J., \& Mané, D. (2016).
\newblock Concrete problems in AI safety.
\newblock \textit{arXiv preprint arXiv:1606.06565}.

\bibitem[Anderson \& Anderson, 2008]{anderson2008geneth}
Anderson, M., \& Anderson, S. L. (2008).
\newblock GenEth: A general ethical dilemma analyzer.
\newblock In \textit{AAAI}, 8, 253--254.

\bibitem[Anderson \& Anderson, 2011]{anderson2011machine}
Anderson, M., \& Anderson, S. L. (2011).
\newblock \textit{Machine Ethics}.
\newblock Cambridge University Press.

\bibitem[Aristotle, 350 BCE]{aristotle350bce}
Aristotle (350 BCE).
\newblock \textit{Nicomachean Ethics}.
\newblock (Trans. W. D. Ross).

\bibitem[Asimov, 1950]{asimov1950robot}
Asimov, I. (1950).
\newblock \textit{I, Robot}.
\newblock Gnome Press.

\bibitem[Bai et al., 2022]{bai2022constitutional}
Bai, Y., Jones, A., Ndousse, K., et al. (2022).
\newblock Training a helpful and harmless assistant with reinforcement learning from human feedback.
\newblock \textit{arXiv preprint arXiv:2204.05862}.

\bibitem[Beauchamp \& Childress, 2001]{beauchamp2001principles}
Beauchamp, T. L., \& Childress, J. F. (2001).
\newblock \textit{Principles of Biomedical Ethics} (5th ed.).
\newblock Oxford University Press.

\bibitem[Berberich \& Diepold, 2015]{berberich2015evolutionary}
Berberich, N., \& Diepold, K. (2015).
\newblock The virtuous machine---Old ethics for new technology?
\newblock \textit{arXiv preprint arXiv:1507.00548}.

\bibitem[Berreby et al., 2015]{berreby2015modelling}
Berreby, F., Bourgne, G., \& Ganascia, J. G. (2015).
\newblock Modelling moral reasoning and ethical responsibility with logic programming.
\newblock In \textit{Logic for Programming, Artificial Intelligence, and Reasoning}, 532--548.

\bibitem[Bhagavad Gita, circa 400 BCE]{bhagavad_gita}
\textit{Bhagavad Gita} (circa 400 BCE).

\bibitem[Bostrom, 2014]{bostrom2014superintelligence}
Bostrom, N. (2014).
\newblock \textit{Superintelligence: Paths, Dangers, Strategies}.
\newblock Oxford University Press.

\bibitem[Brown, 1991]{brown1991human}
Brown, D. E. (1991).
\newblock \textit{Human Universals}.
\newblock McGraw-Hill.

\bibitem[Brown et al., 2020]{brown2020language}
Brown, T. B., Mann, B., Ryder, N., et al. (2020).
\newblock Language models are few-shot learners.
\newblock In \textit{Advances in Neural Information Processing Systems}, 33, 1877--1901.

\bibitem[Bruner, 1991]{bruner1991narrative}
Bruner, J. (1991).
\newblock The narrative construction of reality.
\newblock \textit{Critical Inquiry}, 18(1), 1--21.

\bibitem[Bryson, 2018]{bryson2018patiency}
Bryson, J. J. (2018).
\newblock Patiency is not a virtue: The design of intelligent systems and systems of ethics.
\newblock \textit{Ethics and Information Technology}, 20(1), 15--26.

\bibitem[Cameron, 1991]{cameron1991terminator}
Cameron, J. (Director) (1991).
\newblock \textit{Terminator 2: Judgment Day} [Film].
\newblock TriStar Pictures.

\bibitem[Carruthers, 2015]{carruthers2015conscious}
Carruthers, P. (2015).
\newblock \textit{The Centered Mind: What the Science of Working Memory Shows Us About the Nature of Human Thought}.
\newblock Oxford University Press.

\bibitem[Casper et al., 2023]{casper2023open}
Casper, S., Davies, X., Shi, C., et al. (2023).
\newblock Open problems and fundamental limitations of reinforcement learning from human feedback.
\newblock \textit{arXiv preprint arXiv:2307.15217}.

\bibitem[Chambers \& Jurafsky, 2008]{chambers2008unsupervised}
Chambers, N., \& Jurafsky, D. (2008).
\newblock Unsupervised learning of narrative event chains.
\newblock In \textit{ACL}, 789--797.

\bibitem[Chase \& Simon, 1973]{chase1973perception}
Chase, W. G., \& Simon, H. A. (1973).
\newblock Perception in chess.
\newblock \textit{Cognitive Psychology}, 4(1), 55--81.

\bibitem[Christiano et al., 2017]{christiano2017deep}
Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S., \& Amodei, D. (2017).
\newblock Deep reinforcement learning from human preferences.
\newblock In \textit{Advances in Neural Information Processing Systems}, 4299--4307.

\bibitem[Clarke, 2009]{clarke2009asimov}
Clarke, R. (2009).
\newblock Asimov's laws of robotics: Implications for information technology.
\newblock \textit{Computer}, 26(12), 53--61.

\bibitem[Coeckelbergh, 2010]{coeckelbergh2010moral}
Coeckelbergh, M. (2010).
\newblock Robot rights? Towards a social-relational justification of moral consideration.
\newblock \textit{Ethics and Information Technology}, 12(3), 209--221.

\bibitem[Dennis et al., 2016]{dennis2016formal}
Dennis, L., Fisher, M., Slavkovik, M., \& Webster, M. (2016).
\newblock Formal verification of ethical choices in autonomous systems.
\newblock \textit{Robotics and Autonomous Systems}, 77, 1--14.

\bibitem[Devlin et al., 2018]{devlin2018bert}
Devlin, J., Chang, M. W., Lee, K., \& Toutanova, K. (2018).
\newblock BERT: Pre-training of deep bidirectional transformers for language understanding.
\newblock \textit{arXiv preprint arXiv:1810.04805}.

\bibitem[Dreyfus \& Dreyfus, 2000]{dreyfus2000could}
Dreyfus, H. L., \& Dreyfus, S. E. (2000).
\newblock Mind over machine: The power of human intuition and expertise in the age of the computer.
\newblock Athenaeum.

\bibitem[Emelin et al., 2021]{emelin2021moral}
Emelin, D., Le Bras, R., Hwang, J. D., Forbes, M., \& Choi, Y. (2021).
\newblock Moral stories: Situated reasoning about norms, intents, actions, and their consequences.
\newblock In \textit{EMNLP}, 698--718.

\bibitem[Etzioni \& Etzioni, 2017]{etzioni2017incorporating}
Etzioni, A., \& Etzioni, O. (2017).
\newblock Incorporating ethics into artificial intelligence.
\newblock \textit{The Journal of Ethics}, 21(4), 403--418.

\bibitem[Forbes et al., 2020]{forbes2020social}
Forbes, M., Hwang, J. D., Shwartz, V., Sap, M., \& Choi, Y. (2020).
\newblock Social chemistry 101: Learning to reason about social and moral norms.
\newblock In \textit{EMNLP}, 653--670.

\bibitem[Foucault, 1975]{foucault1975discipline}
Foucault, M. (1975).
\newblock \textit{Discipline and Punish: The Birth of the Prison}.
\newblock Éditions Gallimard.

\bibitem[Garcez et al., 2019]{garcez2019neurosymbolic}
Garcez, A. d'Avila, Gori, M., Lamb, L. C., Serafini, L., Spranger, M., \& Tran, S. N. (2019).
\newblock Neural-symbolic computing: An effective methodology for principled integration of machine learning and reasoning.
\newblock \textit{arXiv preprint arXiv:1905.06088}.

\bibitem[Gentner, 1983]{gentner1983structure}
Gentner, D. (1983).
\newblock Structure-mapping: A theoretical framework for analogy.
\newblock \textit{Cognitive Science}, 7(2), 155--170.

\bibitem[Gips, 1995]{gips1995towards}
Gips, J. (1995).
\newblock Towards the ethical robot.
\newblock In \textit{Second International Workshop on Human and Machine Cognition: Android Epistemology}, 243--252.

\bibitem[Greco, 2010]{greco2010achievements}
Greco, J. (2010).
\newblock \textit{Achieving Knowledge: A Virtue-Theoretic Account of Epistemic Normativity}.
\newblock Cambridge University Press.

\bibitem[Hadfield-Menell et al., 2016]{hadfield2016cooperative}
Hadfield-Menell, D., Russell, S. J., Abbeel, P., \& Dragan, A. (2016).
\newblock Cooperative inverse reinforcement learning.
\newblock In \textit{Advances in Neural Information Processing Systems}, 3909--3917.

\bibitem[Haidt, 2012]{haidt2012righteous}
Haidt, J. (2012).
\newblock \textit{The Righteous Mind: Why Good People Are Divided by Politics and Religion}.
\newblock Vintage.

\bibitem[Hasson et al., 2004]{hasson2004intersubject}
Hasson, U., Nir, Y., Levy, I., Fuhrmann, G., \& Malach, R. (2004).
\newblock Intersubject synchronization of cortical activity during natural vision.
\newblock \textit{Science}, 303(5664), 1634--1640.

\bibitem[Hendrycks et al., 2021a]{hendrycks2021aligning}
Hendrycks, D., Burns, C., Basart, S., et al. (2021).
\newblock Aligning AI with shared human values.
\newblock In \textit{ICLR}.

\bibitem[Hendrycks et al., 2021b]{hendrycks2021unsolved}
Hendrycks, D., Carlini, N., Schulman, J., \& Steinhardt, J. (2021).
\newblock Unsolved problems in ML safety.
\newblock \textit{arXiv preprint arXiv:2109.13916}.

\bibitem[Howard \& Muntean, 2001]{howard2001computational}
Howard, R. A., \& Muntean, I. (2001).
\newblock A computational account of virtue.
\newblock In \textit{Machine Ethics}, 1--12.

\bibitem[Johnson, 1993]{johnson1993moral}
Johnson, M. (1993).
\newblock \textit{Moral Imagination: Implications of Cognitive Science for Ethics}.
\newblock University of Chicago Press.

\bibitem[Kant, 1785]{kant1785groundwork}
Kant, I. (1785).
\newblock \textit{Groundwork of the Metaphysics of Morals}.

\bibitem[Kenton et al., 2021]{kenton2021alignment}
Kenton, Z., Everitt, T., Weidinger, L., Gabriel, I., Mikulik, V., \& Irving, G. (2021).
\newblock Alignment of language agents.
\newblock \textit{arXiv preprint arXiv:2103.14659}.

\bibitem[Lacan, 1949]{lacan1949mirror}
Lacan, J. (1949).
\newblock The mirror stage as formative of the function of the I as revealed in psychoanalytic experience.
\newblock In \textit{Écrits}, 75--81.

\bibitem[Levy, 2014]{levy2014consciousness}
Levy, N. (2014).
\newblock \textit{Consciousness and Moral Responsibility}.
\newblock Oxford University Press.

\bibitem[Li et al., 2013]{li2013story}
Li, B., Lee-Urban, S., Johnston, G., \& Riedl, M. (2013).
\newblock Story generation with crowdsourced plot graphs.
\newblock In \textit{AAAI}.

\bibitem[MacIntyre, 1981]{macintyre1981after}
MacIntyre, A. (1981).
\newblock \textit{After Virtue}.
\newblock University of Notre Dame Press.

\bibitem[Manheim \& Garrabrant, 2018]{manheim2018categorizing}
Manheim, D., \& Garrabrant, S. (2018).
\newblock Categorizing variants of Goodhart's law.
\newblock \textit{arXiv preprint arXiv:1803.04585}.

\bibitem[Minsky, 1974]{minsky1974framework}
Minsky, M. (1974).
\newblock A framework for representing knowledge.
\newblock \textit{MIT-AI Laboratory Memo}, 306.

\bibitem[Minsky, 1986]{minsky1986society}
Minsky, M. (1986).
\newblock \textit{The Society of Mind}.
\newblock Simon \& Schuster.

\bibitem[Minsky, 2006]{minsky2006emotion}
Minsky, M. (2006).
\newblock \textit{The Emotion Machine: Commonsense Thinking, Artificial Intelligence, and the Future of the Human Mind}.
\newblock Simon \& Schuster.

\bibitem[Mitchell \& Krakauer, 2021]{mitchell2021debate}
Mitchell, M., \& Krakauer, D. C. (2021).
\newblock The debate over understanding in AI's large language models.
\newblock \textit{arXiv preprint arXiv:2210.13966}.

\bibitem[Mittelstadt et al., 2019]{mittelstadt2019explaining}
Mittelstadt, B., Russell, C., \& Wachter, S. (2019).
\newblock Explaining explanations in AI.
\newblock In \textit{FAT*}, 279--288.

\bibitem[Moor, 2006]{moor2006nature}
Moor, J. H. (2006).
\newblock The nature, importance, and difficulty of machine ethics.
\newblock \textit{IEEE Intelligent Systems}, 21(4), 18--21.

\bibitem[Mueller, 2003]{mueller2003story}
Mueller, E. T. (2003).
\newblock Story understanding through multi-representation model construction.
\newblock In \textit{HLT-NAACL 2003 Workshop on Text Meaning}.

\bibitem[Ng \& Russell, 2000]{ng2000algorithms}
Ng, A. Y., \& Russell, S. J. (2000).
\newblock Algorithms for inverse reinforcement learning.
\newblock In \textit{ICML}, 663--670.

\bibitem[Nussbaum, 1990]{nussbaum1990love}
Nussbaum, M. C. (1990).
\newblock \textit{Love's Knowledge: Essays on Philosophy and Literature}.
\newblock Oxford University Press.

\bibitem[OpenAI, 2023]{openai2023gpt4}
OpenAI (2023).
\newblock GPT-4 technical report.
\newblock \textit{arXiv preprint arXiv:2303.08774}.

\bibitem[Ouyang et al., 2022]{ouyang2022training}
Ouyang, L., Wu, J., Jiang, X., et al. (2022).
\newblock Training language models to follow instructions with human feedback.
\newblock In \textit{NeurIPS}, 35, 27730--27744.

\bibitem[Pearl, 2009]{pearl2009causality}
Pearl, J. (2009).
\newblock \textit{Causality: Models, Reasoning, and Inference} (2nd ed.).
\newblock Cambridge University Press.

\bibitem[Raman et al., 2022]{raman2022misalignment}
Raman, S. S., Khosla, M., \& Russell, S. (2022).
\newblock Misaligned incentives and human-AI relationships.
\newblock \textit{arXiv preprint arXiv:2210.07461}.

\bibitem[Reagan et al., 2016]{reagan2016emotional}
Reagan, A. J., Mitchell, L., Kiley, D., Danforth, C. M., \& Dodds, P. S. (2016).
\newblock The emotional arcs of stories are dominated by six basic shapes.
\newblock \textit{EPJ Data Science}, 5(1), 31.

\bibitem[Regneri et al., 2010]{regneri2010learning}
Regneri, M., Koller, A., \& Pinkal, M. (2010).
\newblock Learning script knowledge with web experiments.
\newblock In \textit{ACL}, 979--988.

\bibitem[Riedl \& Young, 2010]{riedl2010narrative}
Riedl, M. O., \& Young, R. M. (2010).
\newblock Narrative planning: Balancing plot and character.
\newblock \textit{Journal of Artificial Intelligence Research}, 39, 217--268.

\bibitem[Rossi \& Mattei, 2018]{rossi2018preferences}
Rossi, F., \& Mattei, N. (2018).
\newblock Building ethically bounded AI.
\newblock \textit{arXiv preprint arXiv:1812.03980}.

\bibitem[Russell, 2019]{russell2019human}
Russell, S. (2019).
\newblock \textit{Human Compatible: Artificial Intelligence and the Problem of Control}.
\newblock Viking.

\bibitem[Ryle, 1949]{ryle1949concept}
Ryle, G. (1949).
\newblock \textit{The Concept of Mind}.
\newblock Hutchinson.

\bibitem[Sap et al., 2019]{sap2019socialiqa}
Sap, M., Rashkin, H., Chen, D., LeBras, R., \& Choi, Y. (2019).
\newblock Social IQa: Commonsense reasoning about social interactions.
\newblock In \textit{EMNLP}, 4463--4473.

\bibitem[Schank \& Abelson, 1977]{schank1977scripts}
Schank, R. C., \& Abelson, R. P. (1977).
\newblock \textit{Scripts, Plans, Goals and Understanding: An Inquiry Into Human Knowledge Structures}.
\newblock Lawrence Erlbaum.

\bibitem[Schölkopf et al., 2021]{scholkopf2021toward}
Schölkopf, B., Locatello, F., Bauer, S., et al. (2021).
\newblock Toward causal representation learning.
\newblock \textit{Proceedings of the IEEE}, 109(5), 612--634.

\bibitem[Shepherd, 2018]{shepherd2018consciousness}
Shepherd, J. (2018).
\newblock \textit{Consciousness and Moral Status}.
\newblock Routledge.

\bibitem[Sorensen, 1992]{sorensen1992thought}
Sorensen, R. A. (1992).
\newblock \textit{Thought Experiments}.
\newblock Oxford University Press.

\bibitem[Sosa, 2007]{sosa2007apt}
Sosa, E. (2007).
\newblock \textit{A Virtue Epistemology: Apt Belief and Reflective Knowledge} (Vol. 1).
\newblock Oxford University Press.

\bibitem[Speer et al., 2009]{speer2009reading}
Speer, N. K., Reynolds, J. R., Swallow, K. M., \& Zacks, J. M. (2009).
\newblock Reading stories activates neural representations of visual and motor experiences.
\newblock \textit{Psychological Science}, 20(8), 989--999.

\bibitem[Sutton \& Barto, 2018]{sutton2018reinforcement}
Sutton, R. S., \& Barto, A. G. (2018).
\newblock \textit{Reinforcement Learning: An Introduction} (2nd ed.).
\newblock MIT Press.

\bibitem[Turiel, 1983]{turiel1983development}
Turiel, E. (1983).
\newblock \textit{The Development of Social Knowledge: Morality and Convention}.
\newblock Cambridge University Press.

\bibitem[Vallor, 2016]{vallor2016technology}
Vallor, S. (2016).
\newblock \textit{Technology and the Virtues: A Philosophical Guide to a Future Worth Wanting}.
\newblock Oxford University Press.

\bibitem[Vaswani et al., 2017]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., et al. (2017).
\newblock Attention is all you need.
\newblock In \textit{NeurIPS}, 5998--6008.

\bibitem[Wallach \& Allen, 2008]{wallach2008moral}
Wallach, W., \& Allen, C. (2008).
\newblock \textit{Moral Machines: Teaching Robots Right from Wrong}.
\newblock Oxford University Press.

\bibitem[Webb et al., 2021]{webb2021emergent}
Webb, T., Holyoak, K. J., \& Lu, H. (2021).
\newblock Emergent analogical reasoning in large language models.
\newblock \textit{arXiv preprint arXiv:2212.09196}.

\bibitem[Zwaan \& Radvansky, 2004]{zwaan2004constructionist}
Zwaan, R. A., \& Radvansky, G. A. (2004).
\newblock Situation models in language comprehension and memory.
\newblock \textit{Psychological Bulletin}, 130(2), 162--185.

\end{thebibliography}

\end{document}
